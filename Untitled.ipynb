{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viewing the MovieLens Data\n",
    "Familiarize yourself with the ratings dataset provided here. Would you consider the data to be implicit or explicit ratings?\n",
    "\n",
    "Look at the .columns of the ratings dataframe.\n",
    "Look at the first few rows of ratings dataframe using the .show() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the column names\n",
    "print(ratings.columns)\n",
    "\n",
    "# Look at the first few rows of data\n",
    "print(ratings.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate sparsity\n",
    "As you know, ALS works well with sparse datasets. Let's see how much of the ratings matrix is actually empty.\n",
    "\n",
    "Remember that sparsity is calculated by the number of cells in a matrix that contain a rating divided by the total number of values that matrix could hold given the number of users and items (movies). In other words, dividing the number of ratings present in the matrix by the product of users and movies in the matrix and subtracting that from 1 will give us the sparsity or the percentage of the ratings matrix that is empty.\n",
    "\n",
    "Calculate the numerator of the sparsity metric by counting the total number of ratings that are contained in the ratings matrix.\n",
    "\n",
    "Calculate the number of distinct() userIds and distinct() movieIds in the ratings matrix.\n",
    "\n",
    "Calculate the denominator of the sparsity metric by multiplying the number of users by the number of movies in the ratings matrix.\n",
    "\n",
    "Calculate and print the sparsity by dividing the numerator by the denominator, subtracting from 1 and multiplying by 100. The 1.0 is added to ensure the sparsity is returned as a decimal and not an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the total number of ratings in the dataset\n",
    "numerator = ratings.select(\"rating\").count()\n",
    "\n",
    "# Count the number of distinct userIds and distinct movieIds\n",
    "num_users = ratings.select(\"userId\").distinct().count()\n",
    "num_movies = ratings.select(\"movieId\").distinct().count()\n",
    "\n",
    "# Set the denominator equal to the number of users multiplied by the number of movies\n",
    "denominator = num_users * num_movies\n",
    "\n",
    "# Divide the numerator by the denominator\n",
    "sparsity = (1.0 - (numerator *1.0)/denominator)*100\n",
    "print(\"The ratings dataframe is \", \"%.2f\" % sparsity + \"% empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know a little more about the dataset, let's look at some general summary metrics of the ratings dataset and see how many ratings the movies have and how many ratings each users has provided.\n",
    "\n",
    "Two common methods that will be helpful to you as you aggregate summary statistics in Spark are the .filter() and the .groupBy() methods. The .filter() method allows you to filter out any data that doesn't meet your specified criteria.\n",
    "\n",
    "Import col from the pyspark.sql.functions, and view the ratings dataset using the .show().\n",
    "\n",
    "Apply the .filter() method on the ratings dataset with the following filter inside the parenthesis in order to include only userId's less than 100: col(\"userId\") < 100.\n",
    "\n",
    "Call the .groupBy() method on the ratings dataset to group the data by userId. Call the .count() method to see how many movies each userId has rated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the requisite packages\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# View the ratings dataset\n",
    "ratings.show()\n",
    "\n",
    "# Filter to show only userIds less than 100\n",
    "ratings.filter(col(\"userId\") < 100).show()\n",
    "\n",
    "# Group data by userId, count ratings\n",
    "ratings.groupBy(\"userId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the groupBy() method a bit further.\n",
    "\n",
    "Once you've applied the .groupBy() method to a dataframe, you can subsequently run aggregate functions such as .sum(), .avg(), .min() and have the results grouped. This exercise will walk you through how this is done. The min and avg functions have been imported from pyspark.sql.functions for you.\n",
    "\n",
    "Group the data by movieId and use the .count() method to calculate how many ratings each movie has received. From there, call the .select() method to select the following metrics:\n",
    "\n",
    "min(\"count\") to get the smallest number of ratings that any movie in the dataset. This first one is given to you as an example.\n",
    "\n",
    "avg(\"count\") to get the average number of ratings per movie\n",
    "\n",
    "Do the same thing, but this time group by userId to get the min and avg number of ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min num ratings for movies\n",
    "print(\"Movie with the fewest ratings: \")\n",
    "ratings.groupBy(\"movieId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num ratings per movie\n",
    "print(\"Avg num ratings per movie: \")\n",
    "ratings.groupBy(\"movieId\").count().select(avg(\"count\")).show()\n",
    "\n",
    "# Min num ratings for user\n",
    "print(\"User with the fewest ratings: \")\n",
    "ratings.groupBy(\"userId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num ratings per users\n",
    "print(\"Avg num ratings per user: \")\n",
    "ratings.groupBy(\"userId\").count().select(avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View Schema\n",
    "As you know from previous chapters, Spark's implementation of ALS requires that movieIds and userIds be provided as integer datatypes. Many datasets need to be prepared accordingly in order for them to function properly with Spark. A common issue is that Spark thinks numbers are strings, and vice versa.\n",
    "\n",
    "Here, you'll use the .cast() method to address these types of problems. Let's take a look at the schema of the dataset to ensure it's in the correct format.\n",
    "\n",
    "Use .printSchema() to check whether the ratings dataset contains the proper data types for ALS to function correctly. Are the userIds and movieIds provided as integer datatypes? Are the ratings in numeric format?\n",
    "\n",
    "Ensure that the columns of the ratings dataframe are the correct data types. Call the cast() method on each column and specify the userID and movieId columns to be type \"integer\" and the rating column to be of type \"double\". (We don't need the timestamp column, so we can leave that out.)\n",
    "\n",
    "Call .printSchema() again on ratings to confirm that the data types are now correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .printSchema() to see the datatypes of the ratings dataset\n",
    "ratings.printSchema()\n",
    "\n",
    "# Tell Spark to convert the columns to the proper data types\n",
    "ratings = ratings.select(ratings.userId.cast(\"integer\"), ratings.movieId.cast(\"integer\"), ratings.rating.cast(\"double\"))\n",
    "\n",
    "# Call .printSchema() again to confirm the columns are now in the correct format\n",
    "ratings.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know how to build an ALS model, having done it in the previous chapter. We will do that again, but we'll take some additional steps to fully build out a cross-validated model.\n",
    "\n",
    "First, let's import the requisite functions and create our train and test data sets in preparation for the cross validation step.\n",
    "\n",
    "Import the RegressionEvaluator from ml.evaluation, the ALS algorithm from ml.recommendation, and the ParamGridBuilder and the CrossValidator from ml.tuning.\n",
    "\n",
    "Create an .80/.20 train/test split on the ratings data using the randomSplit method. Name the datasets train and test, and set the random seed to 1234.\n",
    "\n",
    "Build out the ALS model, telling Spark the names of the columns in the ratings dataframe that correspond to the userCol, itemCol and ratingCol. Set the nonnegative argument to True, the coldStartStrategy to \"drop\" and let Spark know that these are not implicitPrefs by setting the implicitPrefs argument to False. Call this model als.\n",
    "Verify that the model was created by calling the type() function on als. The output should indicate what type of model it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required functions\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create test and train set\n",
    "(train, test) = ratings.randomSplit([0.8, 0.2], seed = 1234)\n",
    "\n",
    "# Create ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"itemCol\", ratingCol=\"rating\", nonnegative = True, coldStartStrategy = \"drop\", implicitPrefs = False)\n",
    "\n",
    "# Confirm that a model called \"als\" was created\n",
    "type(als)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll need to create a ParamGrid to tell Spark what hyperparameters we want it to tune, how to tune them, and then build out an evaluator so Spark can know how to measure the algorithm's performance.\n",
    "\n",
    "Import RegressionEvaluator from pyspark.ml.evaluation and ParamGridBuilder and CrossValidator from pyspark.ml.tuning.\n",
    "\n",
    "Build a ParamGrid called param_grid using the ParamGridBuilder provided. Call the .addGrid() method on each hyperparameter by providing the name of the model and the name of each hyperparameter (ex: .addGrid(als.rank, []). Do this for the rank, maxIter and regParam hyperparameters. Also provide the respective lists of hyperparameter values that Spark should try, as provided here:\n",
    " rank: [10, 50, 100, 150]  \n",
    " maxIter: [5, 50, 100, 200]  \n",
    " regParam: [.01, .05, .1, .15]  \n",
    " \n",
    "Create a RegressionEvaluator called evaluator. Set the metricName to \"rmse\", set the labelCol to \"rating\", and tell Spark that when it generates predictions to call the predictionCol \"prediction\".\n",
    "\n",
    "Run len(param_grid) to confirm that the param_grid was created and to confirm that the right number of hyperparameter combinations will be tested. It should be equal to the number of rank values * the number of maxIter values * the number of regParam values in the ParamGridBuilder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the requisite items\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Add hyperparameters and their respective values to param_grid\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 50, 100, 150]) \\\n",
    "            .addGrid(als.maxIter, [5, 50, 100, 200]) \\\n",
    "            .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
    "            .build()\n",
    "           \n",
    "# Define evaluator as RMSE and print length of evaluator\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\") \n",
    "print (\"Num models to be tested: \", len(param_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data, our train/test splits, our model, and our hyperparameter values, let's tell Spark how to cross validate our model so it can find the best combination of hyperparameters and return it to us.\n",
    "\n",
    "Create a CrossValidator called cv with our als model as the estimator, setting estimatorParamMaps to the param_grid you just built. Tell Spark that the evaluator to be used is the \"evaluator\" we built previously. Set the numFolds to 5.\n",
    "\n",
    "Confirm that our cv was built by printing cv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build cross validation using CrossValidator\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Confirm cv was built\n",
    "print(cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our cross validator, cv, built out, we can tell Spark to take our data, fit the ALS algorithm to it, and try the different combinations of hyperparameter values from our param_grid so that it can identify what values provide the smallest RMSE. Unfortunately, this takes too long to complete here, but for your reference, this is how it is done:\n",
    "\n",
    "#Fit cross validator to the 'train' dataset\n",
    "model = cv.fit(train)\n",
    "\n",
    "#Extract best model from the cv model above\n",
    "best_model = model.bestModel\n",
    "This code has been run separately, and the best_model has been identified and saved for you to use. Use the commands given to extract the parameters of the model.\n",
    "\n",
    "Print type(best_model) to confirm that the model ALS built from our hyperparameter options is indeed completed. A print statement is needed here in order to work with subsequent print statements.\n",
    "\n",
    "Extract the rank from the best_model by calling the .getRank() method on the best_model.\n",
    "\n",
    "Extract the maxIter from the best_model by calling the .getMaxIter() method on the best_model.\n",
    "\n",
    "Extract the regParam from the best_model by calling the .getRegParam() method on the best_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print best_model\n",
    "print(type(best_model))\n",
    "\n",
    "# Complete the code below to extract the ALS model parameters\n",
    "print(\"**Best Model**\")\n",
    "\n",
    "# Print \"Rank\"\n",
    "print(\"  Rank:\", best_model.getRank())\n",
    "\n",
    "# Print \"MaxIter\"\n",
    "print(\"  MaxIter:\", best_model.getMaxIter())\n",
    "\n",
    "# Print \"RegParam\"\n",
    "print(\"  RegParam:\", best_model.getRegParam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate predictions and calculate RMSE\n",
    "Now that we have a model that is trained on our data and tuned through cross validation, we can see how it performs on the test dataframe. To do this, we'll calculate the RMSE.\n",
    "\n",
    "As a side note, the generation of test predictions takes more than a few minutes with this dataset. For this reason, the test predictions have been generated already and are provided here as a dataframe called test_predictions. For your reference, they are generated using this code: test_predictions = best_model.transform(test).\n",
    "\n",
    "The dataframe test_predictions contains predictions that our cross-validated ALS model generated using the test set that we created previously. Use the .show() method to take a look at it and see if the predictions seem close.\n",
    "\n",
    "Use the evaluator that you built previously to calculate the RMSE by calling the .evaluate() method on the test_predictions generated. Call this RMSE.\n",
    "\n",
    "Print the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the predictions \n",
    "test_predictions.show()\n",
    "\n",
    "# Calculate and print the RMSE of test_predictions\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an understanding of how well our model performed, and have some confidence that it will provide recommendations that are relevant to users, let's actually look at recommendations made to a user and see if they make sense.\n",
    "\n",
    "The original ratings data is provided here as original_ratings. Take a look at user 60 and user 63's original ratings, and compare them to what ALS recommended for them. In your opinion, are the recommendations consistent with their original preferences?\n",
    "\n",
    "Use the .filter() on the original_ratings dataframe to ensure that col(\"userId\") equals 60 to look at user 60's original ratings. Call the .sort() method to sort the output by rating and set the ascending argument to False to see the highest ratings first.\n",
    "\n",
    "Use the .filter() and .show() methods on the recommendations dataset to look at user 60's ratings. Note that when ALS generates recommendations, they are provided in descending order by userId, so there's no need to sort the dataframe.\n",
    "\n",
    "Do the same thing for user 63.\n",
    "\n",
    "Do the recommendation genres have some consistency with each user's original ratings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at user 60's ratings\n",
    "print(\"User 60's Ratings:\")\n",
    "original_ratings.filter(col(\"userId\") == 60).sort(\"rating\", ascending = False).show()\n",
    "\n",
    "# Look at the movies recommended to user 60\n",
    "print(\"User 60s Recommendations:\")\n",
    "recommendations.filter(col(\"userId\") == 60).show()\n",
    "\n",
    "# Look at user 63's ratings\n",
    "print(\"User 63's Ratings:\")\n",
    "original_ratings.filter(col(\"userId\") == 63).sort(\"rating\", ascending = False).show()\n",
    "\n",
    "# Look at the movies recommended to user 63\n",
    "print(\"User 63's Recommendations:\")\n",
    "recommendations.filter(col(\"userId\") == 63).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get familiar with the Million Songs Echo Nest Taste Profile data subset. For purposes of this course, we'll just call it the Million Songs dataset or msd. Let's get the number of users and the number of songs. Let's also see which songs have the most plays from this subset.\n",
    "\n",
    "Use the .show() method to see what the data looks like.\n",
    "\n",
    "Complete the code to count the number of distinct userIds. Select the userId column, then call .distinct() and .count().\n",
    "\n",
    "Now do the same thing for the songIds, so count the number of distinct songIds. Select the songId column and call .distinct() and .count() on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data\n",
    "msd.show()\n",
    "\n",
    "# Count the number of distinct userIds\n",
    "user_count = msd.select(\"userId\").distinct().count()\n",
    "print(\"Number of users: \", user_count)\n",
    "\n",
    "# Count the number of distinct songIds\n",
    "song_count = msd.select(\"songId\").distinct().count()\n",
    "print(\"Number of songs: \", song_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are going to combine the .groupBy() and .filter() methods that you've used previously to calculate the min() and avg() number of users that have rated each song, and the min() and avg() number of songs that each user has rated.\n",
    "\n",
    "Because our data now includes 0's for items not yet consumed, we'll need to .filter() them out when doing grouped summary statistics like this. The msd dataset is provided for you here. The col(), min(), and avg() functions from pyspark.sql.functions have been imported for you.\n",
    "\n",
    "As an example, the .filter(), .groupBy() and .count() methods are applied to the msd dataset along with .select() and min() to return the smallest number of ratings that any song in the dataset has received. Use this as a model to calculate the avg() number of implicit ratings the songs in msd have received.\n",
    "\n",
    "Using the same model, find the min() and avg() number of implicit ratings that userIds have provided in the msd dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min num implicit ratings for a song\n",
    "print(\"Minimum implicit ratings for a song: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"songId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings per songs\n",
    "print(\"Average implicit ratings per song: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"songId\").count().select(avg(\"count\")).show()\n",
    "\n",
    "# Min num implicit ratings from a user\n",
    "print(\"Minimum implicit ratings from a user: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"userId\").count().select(min(\"count\")).show()\n",
    "\n",
    "# Avg num implicit ratings for users\n",
    "print(\"Average implicit ratings per user: \")\n",
    "msd.filter(col(\"num_plays\") > 0).groupBy(\"userId\").count().select(avg(\"count\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many recommendation engines use implicit ratings. In many cases these datasets don't include behavior counts for items that a user has never purchased. In these cases, you'll need to add them and include zeros. The dataframe Z is provided for you. It contains userId's, productId's and num_purchases which is the number of times a user has purchased a specific product.\n",
    "\n",
    "Take a look at the dataframe Z using the .show() method.\n",
    "\n",
    "Extract the distinct userIds and productIds from Z using the .distinct() method. Call the results users and products respectively.\n",
    "\n",
    "Perform a .crossJoin() on the users and products dataframes. Call the result cj.\n",
    "\n",
    "\"left\" join cj to the original ratings dataframe Z on [\"userId\", \"productId\"]. Call the .fillna(0) method on the result to fill in the blanks with zeros. Call the result Z_expanded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the data\n",
    "Z.show()\n",
    "\n",
    "# Extract distinct userIds and productIds\n",
    "users = Z.select(\"userId\").distinct()\n",
    "products = Z.select(\"productId\").distinct()\n",
    "\n",
    "# Cross join users and products\n",
    "cj = users.crossJoin(products)\n",
    "\n",
    "# Join cj and Z\n",
    "Z_expanded = cj.join(Z, [\"userId\", \"productId\"], \"left\").fillna(0)\n",
    "\n",
    "# View Z_expanded\n",
    "Z_expanded.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're now going to build your first implicit rating recommendation engine using ALS. To do this, you will first tell Spark what values you want it to try when finding the best model.\n",
    "\n",
    "Four empty lists are provided below. You will fill them with specific values that Spark can use to build several different ALS models. In the next exercise, you'll tell Spark to build out these models using the lists below.\n",
    "\n",
    "Fill in the following lists with the following values:\n",
    "\n",
    "ranks = [10, 20, 30, 40]\n",
    "\n",
    "maxIters = [10, 20, 30, 40]\n",
    "\n",
    "regParams = [.05, .1, .15]\n",
    "\n",
    "alphas = [20, 40, 60, 80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the lists below\n",
    "ranks = [10, 20, 30, 40]\n",
    "maxIters = [10, 20, 30, 40]\n",
    "regParams = [.05, .1, .15]\n",
    "alphas = [20, 40, 60, 80]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have all of your hyperparameter values specified, let's have Spark build enough models to test each combination. To facilitate this, a for loop is provided here. Follow the instructions below to automatically create these ALS models. In subsequent exercises you will run these models on test datasets to see which one performs the best.\n",
    "\n",
    "The ALS algorithm is already imported for you. The lists you created in the last exercise (ranks, maxIters, regParams, alphas) have been created for you.\n",
    "\n",
    "An empty list called model_list is provided here. The for loop will create a model for each combination of hyperparameters it is given and put it into model_list.\n",
    "\n",
    "Complete the for loop by referencing the contents of each list where r represents the elements of the ranks list, mi represents the elements of the maxIters list, rp represents the elements of the regParams list and a represents the elements of the alphas list.\n",
    "\n",
    "Print len(model_list) as well as model_list to ensure that each model was created. The length should be equal to the product of all the lengths of each hyperparameter list above. You can run the additional validation provided to be sure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loop will automatically create and store ALS models\n",
    "for r in ranks:\n",
    "    for mi in maxIters:\n",
    "        for rp in regParams:\n",
    "            for a in alphas:\n",
    "                model_list.append(ALS(userCol= \"userId\", itemCol= \"songId\", ratingCol= \"num_plays\", rank = r, maxIter = mi, regParam = rp, alpha = a, coldStartStrategy=\"drop\", nonnegative = True, implicitPrefs = True))\n",
    "\n",
    "# Print the model list, and the length of model_list\n",
    "print (model_list, \"Length of model_list: \", len(model_list))\n",
    "\n",
    "# Validate\n",
    "len(model_list) == (len(ranks)*len(maxIters)*len(regParams)*len(alphas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have several ALS models, each with a different set of hyperparameter values, we can train them on a training portion of the msd dataset using cross validation, and then run them on a test set of data and evaluate how well each one performs using the ROEM function discussed earlier. Unfortunately, this takes too much time for this exercise, so it has been done separately. But for your reference you can evaluate your model_list using the following loop (we are using the msd dataset in this case):\n",
    "\n",
    "Split the data into training and test sets\n",
    "(training, test) = msd.randomSplit([0.8, 0.2])\n",
    "\n",
    "#Building 5 folds within the training set.\n",
    "train1, train2, train3, train4, train5 = training.randomSplit([0.2, 0.2, 0.2, 0.2, 0.2], seed = 1)\n",
    "fold1 = train2.union(train3).union(train4).union(train5)\n",
    "fold2 = train3.union(train4).union(train5).union(train1)\n",
    "fold3 = train4.union(train5).union(train1).union(train2)\n",
    "fold4 = train5.union(train1).union(train2).union(train3)\n",
    "fold5 = train1.union(train2).union(train3).union(train4)\n",
    "\n",
    "foldlist = [(fold1, train1), (fold2, train2), (fold3, train3), (fold4, train4), (fold5, train5)]\n",
    "\n",
    "Empty list to fill with ROEMs from each model\n",
    "ROEMS = []\n",
    "\n",
    "Loops through all models and all folds\n",
    "for model in model_list:\n",
    "    for ft_pair in foldlist:\n",
    "\n",
    "        Fits model to fold within training data\n",
    "        fitted_model = model.fit(ft_pair[0])\n",
    "\n",
    "        Generates predictions using fitted_model on respective CV test data\n",
    "        predictions = fitted_model.transform(ft_pair[1])\n",
    "\n",
    "        Generates and prints a ROEM metric CV test data\n",
    "        r = ROEM(predictions)\n",
    "        print (\"ROEM: \", r)\n",
    "\n",
    "    Fits model to all of training data and generates preds for test data\n",
    "    v_fitted_model = model.fit(training)\n",
    "    v_predictions = v_fitted_model.transform(test)\n",
    "    v_ROEM = ROEM(v_predictions)\n",
    "\n",
    "    Adds validation ROEM to ROEM list\n",
    "    ROEMS.append(v_ROEM)\n",
    "    print (\"Validation ROEM: \", v_ROEM)\n",
    "For purposes of walking you through the steps, the test predictions for 192 models have already been generated, and their ROEM has been calculated. They are found in the ROEMS list provided. Because a list isn't unique to Pyspark, and because numpy works really well with lists, we're going to use numpy here. Follow the instructions below to find the best ROEM and the model that provided it.\n",
    "\n",
    "Import numpy.\n",
    "\n",
    "Extract the smallest ROEM from the ROEMS list provided using numpy.argmin(). The .argmin() method will return the index of the lowest value in the list provided. Call the result i and print i.\n",
    "\n",
    "Use list slicing to find the value in the ROEMS list at index i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy\n",
    "\n",
    "# Find the index of the smallest ROEM\n",
    "i = numpy.argmin(ROEMS)\n",
    "print(\"Index of smallest ROEM:\", i)\n",
    "\n",
    "# Find ith element of ROEMS\n",
    "print(\"Smallest ROEM: \", ROEMS[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've now tested 192 different models on the msd dataset, and you found the best ROEM and its respective model (model 38).\n",
    "\n",
    "You now need to extract the hyperparameters. The model_list you created previously is provided here. It contains all 192 models you generated. Use the instructions below to extract the hyperparameters.\n",
    "\n",
    "Use list slicing to extract model 38 from the model list. Call this model best_model.\n",
    "\n",
    "Use the .get____() method to extract the following hyperparameter values:\n",
    "\n",
    "Rank\n",
    "\n",
    "MaxIter\n",
    "\n",
    "RegParam\n",
    "\n",
    "Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the best_model\n",
    "best_model = model_list[38]\n",
    "\n",
    "# Extract the Rank\n",
    "print (\"Rank: \", best_model.getRank())\n",
    "\n",
    "# Extract the MaxIter value\n",
    "print (\"MaxIter: \", best_model.getMaxIter())\n",
    "\n",
    "# Extract the RegParam value\n",
    "print (\"RegParam: \", best_model.getRegParam())\n",
    "\n",
    "# Extract the Alpha value\n",
    "print (\"Alpha: \", best_model.getAlpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've already built several ALS models, so we won't do that again. An implicit ALS model has already been fitted to the binary ratings of the MovieLens dataset. Let's look at the binary_test_predictions from this model to see what we can learn.\n",
    "\n",
    "The ROEM() function has been defined for you. Feel free to run help(ROEM) in the console if you want more details on how to execute it!\n",
    "\n",
    "Import the col function from the pyspark.sql.functions class.\n",
    "\n",
    "Take a look at binary_test_predictions using the .show() method to get an understanding of what the data looks like.\n",
    "\n",
    "Call ROEM() on the binary_test_predictions to evaluate the model's performance. Do you think it performed well?\n",
    "\n",
    "Use .filter() to look only at user 42's predictions (col(\"userId\") == 42). Do you notice anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the col function\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Look at the test predictions\n",
    "binary_test_predictions.show()\n",
    "\n",
    "# Evaluate ROEM on test predictions\n",
    "ROEM(binary_test_predictions)\n",
    "\n",
    "# Look at user 42's test predictions\n",
    "binary_test_predictions.filter(col(\"userId\") == 42).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So you see from the ROEM, these models can still generate meaningful test predictions. Let's look at the actual recommendations now.\n",
    "\n",
    "The col function from the pyspark.sql.functions class has been imported for you.\n",
    "\n",
    "The dataframe original_ratings is provided for you. It contains the original ratings that users provided. Use .filter() to examine user 26's original ratings.\n",
    "\n",
    "The dataframe binary_recs is also provided. It contains the recommendations for each user. Use .filter() to examine userId == 26 recommendations. Do they seem consistent with the recommendations the model provided?\n",
    "\n",
    "Do the same thing with user 99. Feel free to examine other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View user 26's original ratings\n",
    "print (\"User 26 Original Ratings:\")\n",
    "original_ratings.filter(col(\"userId\") == 26).show()\n",
    "\n",
    "# View user 26's recommendations\n",
    "print (\"User 26 Recommendations:\")\n",
    "binary_recs.filter(col(\"userId\") == 26).show()\n",
    "\n",
    "# View user 99's original ratings\n",
    "print (\"User 99 Original Ratings:\")\n",
    "original_ratings.filter(col(\"userId\") == 99).show()\n",
    "\n",
    "# View user 99's recommendations\n",
    "print (\"User 99 Recommendations:\")\n",
    "binary_recs.filter(col(\"userId\") == 99).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1, 1]\n",
    "y = [2, 2]\n",
    "\n",
    "print(x == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
